{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter notebook (called project.ipynb) that can be run directly and that\n",
    "demonstrates your project. Your notebook can import a sample of the data that\n",
    "you used, import 1 or more models that you built, and generate examples of the\n",
    "types of predictions or simulations your model can make. The notebook should\n",
    "not take any longer than 1 minute to run in total (if you have models that\n",
    "require a lot of training time, train them offline and just upload the models and\n",
    "some sample data to illustrate them). Feel free to generate examples of your\n",
    "model(s) in action, e.g., for reviews you could generate examples of reviews\n",
    "where the models work well and reviews where the models work poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's import the necessary libraries and create our custom Tetris environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.env.tetris_env import TetrisEnv\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Environment\n",
    "\n",
    "The custom `TetrisEnv` wrapper includes:\n",
    "- **Reward Engineering**: Custom rewards for line clears, penalties for holes/bumpiness\n",
    "- **State Analysis**: Tracks holes, column heights, and bumpiness\n",
    "- **Game Statistics**: Monitors lines cleared, episode steps, and total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = TetrisEnv(render_mode=None)\n",
    "\n",
    "# Display environment information\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"\\nNumber of possible actions: {env.action_space.n}\")\n",
    "print(\"\\n✓ Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Environment with Random Agent\n",
    "\n",
    "Let's run a short episode with random actions to demonstrate the environment functionality and reward engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Run episode\n",
    "episode_data = {\n",
    "    'rewards': [],\n",
    "    'holes': [],\n",
    "    'heights': [],\n",
    "    'bumpiness': []\n",
    "}\n",
    "\n",
    "print(\"Running random agent for 50 steps or until game over...\")\n",
    "for step in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    episode_data['rewards'].append(reward)\n",
    "    episode_data['holes'].append(info['holes'])\n",
    "    episode_data['heights'].append(info['max_height'])\n",
    "    episode_data['bumpiness'].append(info['bumpiness'])\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode ended at step {step + 1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n=== Episode Summary ===\")\n",
    "print(f\"Total steps: {len(episode_data['rewards'])}\")\n",
    "print(f\"Total reward: {sum(episode_data['rewards']):.2f}\")\n",
    "print(f\"Lines cleared: {env.total_lines_cleared}\")\n",
    "print(f\"Final holes: {episode_data['holes'][-1]}\")\n",
    "print(f\"Final max height: {episode_data['heights'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Episode Metrics\n",
    "\n",
    "Visualize how the game state evolved during the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Reward over time\n",
    "axes[0, 0].plot(episode_data['rewards'], color='blue')\n",
    "axes[0, 0].set_title('Reward per Step')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative reward\n",
    "cumulative_reward = np.cumsum(episode_data['rewards'])\n",
    "axes[0, 1].plot(cumulative_reward, color='green')\n",
    "axes[0, 1].set_title('Cumulative Reward')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Cumulative Reward')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Holes over time\n",
    "axes[1, 0].plot(episode_data['holes'], color='red')\n",
    "axes[1, 0].set_title('Holes in Board')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Number of Holes')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Max height over time\n",
    "axes[1, 1].plot(episode_data['heights'], color='purple')\n",
    "axes[1, 1].set_title('Maximum Stack Height')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Height')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Final Board State\n",
    "\n",
    "Display the final state of the Tetris board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = obs['board']\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.imshow(board, cmap='gray_r', interpolation='nearest')\n",
    "plt.title('Final Board State')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "plt.colorbar(label='Cell Value')\n",
    "plt.grid(True, which='both', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Board dimensions: {board.shape}\")\n",
    "print(f\"Filled cells: {np.count_nonzero(board)} / {board.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that the environment is set up and tested, we can proceed to:\n",
    "\n",
    "1. **Phase 2**: Build the CNN architecture (policy and value networks)\n",
    "2. **Phase 3**: Implement the PPO training algorithm\n",
    "3. **Phase 4**: Train the agent and evaluate performance\n",
    "\n",
    "The environment is ready for RL training with custom reward engineering that encourages:\n",
    "- Clearing lines (positive reward)\n",
    "- Avoiding holes (penalty)\n",
    "- Keeping the stack low (penalty for height)\n",
    "- Maintaining even column heights (penalty for bumpiness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
